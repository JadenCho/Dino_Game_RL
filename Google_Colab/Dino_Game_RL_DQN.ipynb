{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dino_Game_RL_DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS62i8xNJuFQ"
      },
      "outputs": [],
      "source": [
        "!pip install selenium > /dev/null\n",
        "!apt-get update > /dev/null # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver> /dev/null\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from gym import spaces\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import base64\n",
        "import cv2\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import math"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QNEdtDy4VQC",
        "outputId": "6caf447c-9f5a-4918-f070-f1d631503ff8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Game():\n",
        "\tdef __init__(self):\n",
        "\t\tself.chrome_options = webdriver.ChromeOptions()\n",
        "\t\t#self.chrome_options.binary_location = chrome_path # File path where chrome.exe is\n",
        "\t\tself.chrome_options.add_argument(\"--mute-audio\")\n",
        "\t\tself.chrome_options.add_argument(\"--headless\")\n",
        "\t\tself.chrome_options.add_argument('--no-sandbox')\n",
        "\t\tself.chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\t\t#self.chrome_options.setExperimentalOption(\"excludeSwitches\",Arrays.asList(\"disable-popup-blocking\"));\n",
        "\t\tself.chrome_options.add_argument('start-maximized')\n",
        "\t\tself.driver = webdriver.Chrome('chromedriver', options=self.chrome_options)\n",
        "\n",
        "\tdef Start(self):\n",
        "\t\t'''\n",
        "\t\tOpen the Game Instance in Chrome\n",
        "\t\t'''\n",
        "\t\tself.driver.get('https://chromedino.com/')\n",
        "\n",
        "\tdef Action(self, action):\n",
        "\t\t'''\n",
        "\t\tPerform action\n",
        "\t\t'''\n",
        "\t\tself.driver.find_element(By.TAG_NAME, 'body').send_keys(action)\n",
        "\n",
        "\tdef Refresh(self):\n",
        "\t\t'''\n",
        "\t\tRefresh the Chrome Tab\n",
        "\t\t'''\n",
        "\t\tself.driver.refresh()\n",
        "\t\tWebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"runner-canvas\")))\n",
        "\n",
        "\tdef Restart(self):\n",
        "\t\t'''\n",
        "\t\tRefresh the Chrome Tab and start the game again\n",
        "\t\t'''\n",
        "\t\tself.driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.SPACE)\n",
        "\n",
        "\tdef Get_Score(self):\n",
        "\t\t'''\n",
        "\t\tReturn the score of the gane\n",
        "\t\t'''\n",
        "\t\tscore = self.driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
        "\t\tscore = ''.join(score)\n",
        "\n",
        "\t\treturn int(score)\n",
        "\n",
        "\tdef Img_State(self):\n",
        "\t\t'''\n",
        "\t\tReturn the image of the current state\n",
        "\t\t'''\n",
        "\t\timg = self.driver.execute_script(\"return document.querySelector('canvas.runner-canvas').toDataURL()\")\n",
        "\t\treturn img\n",
        "\n",
        "\tdef Done_State(self):\n",
        "\t\t'''\n",
        "\t\tReturn whether the dino has crashed or not\n",
        "\t\t'''\n",
        "\t\tdone = self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
        "\t\t\n",
        "\t\treturn done\n",
        "\n",
        "class DinoEnv(gym.Env):\n",
        "\tdef __init__(self, width=120, height=120, chrome_path=None):\n",
        "\t\tself.screen_width = width\n",
        "\t\tself.screen_height = height\n",
        "\n",
        "\t\tself.action_space = spaces.Discrete(3) # Do nothing, jump, crouch\n",
        "\t\tself.observation_space = spaces.Box(low=0, high=255, shape=(self.screen_width, self.screen_height, 4), dtype=np.uint8)\n",
        "\n",
        "\t\tself.state_queue = deque(maxlen=4)\n",
        "\n",
        "\t\tself.game = Game()\n",
        "\n",
        "\t\tself.action_list = [Keys.ARROW_LEFT, Keys.ARROW_UP, Keys.ARROW_DOWN]\t\t\n",
        "\n",
        "\tdef Env_Start(self):\n",
        "\t\t'''\n",
        "\t\tStart the Dino Game Instance\n",
        "\t\t'''\n",
        "\t\tself.game.Start()\n",
        "\n",
        "\tdef step(self, action):\n",
        "\t\t'''\n",
        "\t\tReturns Observation, reward, done, other\n",
        "\t\t'''\n",
        "\t\tself.game.Action(self.action_list[action])\n",
        "\n",
        "\t\tnext_state = self.next_state()\n",
        "\n",
        "\t\tdone = self.done_state()\n",
        "\n",
        "\t\treward = 1 if not done else -100\n",
        "\n",
        "\t\tscore = self.game.Get_Score()\n",
        "\t\t#print(score)\n",
        "\n",
        "\t\ttime.sleep(0.15)\n",
        "\n",
        "\t\treturn next_state, reward, done, score\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\t'''\n",
        "\t\tReset the Dino Game Instance\n",
        "\t\t'''\n",
        "\t\tself.game.Restart()\n",
        "\n",
        "\t\treturn self.next_state()\n",
        "\n",
        "\tdef get_state_img(self):\n",
        "\t\t'''\n",
        "\t\tReturns an image of the current state of the game\n",
        "\t\t'''\n",
        "\t\tLEADING_TEXT = \"data:image/png;base64,\"\n",
        "\t\timg = self.game.Img_State()\n",
        "\t\timg = img[len(LEADING_TEXT):]\n",
        "\n",
        "\t\treturn np.array(Image.open(BytesIO(base64.b64decode(img))))\n",
        "\n",
        "\tdef next_state(self):\n",
        "\t\t'''\n",
        "\t\tProcesses the image of the state\n",
        "\t\t'''\n",
        "\t\timg = cv2.cvtColor(self.get_state_img(), cv2.COLOR_BGR2GRAY)\n",
        "\t\timg = img[:, :150] # Cropping\n",
        "\t\timg = cv2.resize(img, (self.screen_width, self.screen_height)) # Resize\n",
        "\n",
        "\t\tself.state_queue.append(img)\n",
        "\n",
        "\t\tif len(self.state_queue) < 4:\n",
        "\t\t\treturn np.stack([img] * 4, axis=-1)\n",
        "\t\telse:\n",
        "\t\t\treturn np.stack(self.state_queue, axis=-1)\n",
        "\t\t#return img\n",
        "\n",
        "\tdef Score(self):\n",
        "\t\t'''\n",
        "\t\tObtain and return score from the Game Instance\n",
        "\t\t'''\n",
        "\t\tscore = self.game.Get_Score()\n",
        "\t\treturn score\n",
        "\n",
        "\tdef done_state(self):\n",
        "\t\t'''\n",
        "\t\tCheck and return whether the Dino has crashed or not\n",
        "\t\t'''\n",
        "\t\treturn self.game.Done_State()"
      ],
      "metadata": {
        "id": "cILGLW5R4YDZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# policy network\n",
        "def OurModel(input_shape, action_space):\n",
        "\n",
        "    input = tf.keras.layers.Input(input_shape)\n",
        "    s = input\n",
        "\n",
        "    c1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3),padding='same',activation='relu', activity_regularizer='L1L2')(s)\n",
        "    c1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3),padding='same',activation='relu')(c1)\n",
        "    do1 = tf.keras.layers.Dropout(0.15)(c1)\n",
        "\n",
        "    m1 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(do1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3),padding='same',activation='relu', activity_regularizer='L1L2')(m1)\n",
        "    c2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3),padding='same',activation='relu')(c2)\n",
        "    do2 = tf.keras.layers.Dropout(0.15)(c2)\n",
        "\n",
        "    m2 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(do2)\n",
        "\n",
        "    c3 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3),padding='same',activation='relu', activity_regularizer='L1L2')(m2)\n",
        "    c3 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3),padding='same',activation='relu')(c3)\n",
        "    c3 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3),padding='same',activation='relu')(c3)\n",
        "    do3 = tf.keras.layers.Dropout(0.15)(c3)\n",
        "\n",
        "    m3 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(do3)\n",
        "\n",
        "    c4 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3),padding='same',activation='relu', activity_regularizer='L1L2')(m3)\n",
        "    c4 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3),padding='same',activation='relu')(c4)\n",
        "    c4 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3),padding='same',activation='relu')(c4)\n",
        "    do4 = tf.keras.layers.Dropout(0.15)(c4)\n",
        "\n",
        "    m4 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(do4)\n",
        "\n",
        "    c5 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3),padding='same',activation='relu', activity_regularizer='L1L2')(m4)\n",
        "    c5 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3),padding='same',activation='relu')(c5)\n",
        "    c5 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3),padding='same',activation='relu')(c5)\n",
        "    do5 = tf.keras.layers.Dropout(0.15)(c5)\n",
        "\n",
        "    f1 = tf.keras.layers.Flatten()(do5)\n",
        "\n",
        "    d1 = tf.keras.layers.Dense(units=4096, activation='relu')(f1)\n",
        "    d2 = tf.keras.layers.Dense(units=1024, activation='relu')(d1)\n",
        "\n",
        "    d = tf.keras.layers.Dense(units=action_space, activation='linear')(d2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input], outputs=[d])\n",
        "    \n",
        "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"]) \n",
        "\n",
        "    # model.summary()\n",
        "    return model\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.env = DinoEnv()\n",
        "        self.state_size = self.env.observation_space.shape\n",
        "        #self.state_size = (120, 120, 4)\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.EPISODES = 100 \n",
        "        self.memory = deque(maxlen=2000)\n",
        "        \n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.001\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.batch_size = 32\n",
        "        self.train_start = 500\n",
        "\n",
        "        # create main model\n",
        "        self.Target_model = OurModel(input_shape=self.state_size, action_space = self.action_size) \n",
        "        self.Train_model = OurModel(input_shape=self.state_size, action_space = self.action_size) \n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.train_start:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "    # to do\n",
        "    # implement the epsilon-greedy policy\n",
        "    def act(self, state):\n",
        "        p = np.random.uniform() \n",
        "        if p < self.epsilon: \n",
        "          action = self.env.action_space.sample() \n",
        "        else: \n",
        "          q = self.Train_model.predict(state[np.newaxis,:]) \n",
        "          action = np.argmax(q) \n",
        "        return action \n",
        "\n",
        "    # to do\n",
        "    # implement the Q-learning\n",
        "    def replay(self): \n",
        "        if len(self.memory) < self.train_start:\n",
        "            return\n",
        "        # Randomly sample minibatch from the memory\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
        "\n",
        "        #state = np.zeros((self.batch_size, self.state_size))\n",
        "        state = np.zeros((self.batch_size, 120, 120, 4))\n",
        "        #next_state = np.zeros((self.batch_size, self.state_size))\n",
        "        next_state = np.zeros((self.batch_size, 120, 120, 4))\n",
        "        action, reward, done, targets = [], [], [], [] \n",
        "\n",
        "        # assign data into state, next_state, action, reward and done from minibatch\n",
        "        for i in range(self.batch_size):\n",
        "            state[i] = minibatch[i][0] \n",
        "            next_state[i] = minibatch[i][3] \n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            done.append(minibatch[i][4]) \n",
        "\n",
        "        # compute value function of current state (call it target) and value function of next state (call it target_next)\n",
        "        for i in range(self.batch_size):\n",
        "            target = self.Train_model.predict(state[i][np.newaxis,:]) \n",
        "            target = target[0] \n",
        "            target_next = self.Target_model.predict(next_state[i][np.newaxis,:]) \n",
        "            target_next = target_next[0] \n",
        "\n",
        "            # correction on the Q value for the action used,\n",
        "            # if done[i] is true, then the target should be just the final reward\n",
        "            if not done[i]:\n",
        "                # else, use Bellman Equation\n",
        "                # Standard - DQN\n",
        "                # DQN chooses the max Q value among next actions\n",
        "                # selection and evaluation of action is on the target Q Network\n",
        "                # target = max_a' (r + gamma*Q_target_next(s', a'))\n",
        "\n",
        "                q_next = np.max(target_next) \n",
        "                new_q = reward[i] + self.gamma*q_next \n",
        "            else:\n",
        "                new_q = reward[i] \n",
        "\n",
        "            target[action[i]] = new_q\n",
        "            targets.append(target) \n",
        "\n",
        "        # Train the Neural Network with batches where target is the value function\n",
        "        targets = np.asarray(targets) \n",
        "        self.Train_model.fit(state, targets, batch_size=self.batch_size, verbose=0) \n",
        "\n",
        "    def load(self, name):\n",
        "        self.model = load_model(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)\n",
        "            \n",
        "    def training(self):\n",
        "        max = 0\n",
        "        total_r = [] \n",
        "        count = 25 \n",
        "        start = time.time() \n",
        "        self.env.Env_Start()\n",
        "\n",
        "        for e in range(self.EPISODES):\n",
        "            time.sleep(1.5)\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            i = 0\n",
        "            \n",
        "            while not done:\n",
        "                # if you have graphic support, you can render() to see the animation. \n",
        "                #self.env.render()\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                    \n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                \n",
        "                i += 1\n",
        "                if done:  \n",
        "                    if i > max:\n",
        "                      max = i \n",
        "                    dateTimeObj = datetime.now()\n",
        "                    timestampStr = dateTimeObj.strftime(\"%H:%M:%S\")\n",
        "\n",
        "                    end = time.time() \n",
        "                    elapse = np.abs(start-end) \n",
        "                    hour = elapse/3600\n",
        "                    minute = np.abs(elapse - (math.floor(hour) * 3600))/60 \n",
        "                    seconds = np.abs(minute - math.floor(minute)) * 60 \n",
        "                    print(f\"\\repisode: {e+1}/{self.EPISODES}, score: {i}, max score: {max}, e: {round(self.epsilon, 4)}, time: {timestampStr}, elapsed time: {math.floor(hour)} hours, {math.floor(minute)} minutes, {math.floor(seconds)} seconds\", end='', flush=True) \n",
        "                    #print(f\"episode: {e+1}/{self.EPISODES}, score: {i}, max score: {max}, e: {round(self.epsilon, 4)}, time: {timestampStr}, elapsed time: {math.floor(hour)} hours, {math.floor(minute)} minutes, {math.floor(seconds)} seconds\") \n",
        "                    total_r.append(i) \n",
        "\n",
        "                    self.replay() \n",
        "                    if e > count: \n",
        "                      count += e \n",
        "                      self.Target_model.set_weights(self.Train_model.get_weights()) \n",
        "\n",
        "        epi = np.linspace(0, self.EPISODES, self.EPISODES) \n",
        "        plt.plot(epi, total_r) \n",
        "        plt.xlabel('Episodes') \n",
        "        plt.ylabel('Total Reward') \n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tagent = DQNAgent()\n",
        "\tagent.training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BW8YdE04pGq",
        "outputId": "135842df-9164-4ab8-f5f5-990aede6b43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 25/100, score: 22, max score: 30, e: 0.8913, time: 22:08:00, elapsed time: 0 hours, 5 minutes, 5 seconds"
          ]
        }
      ]
    }
  ]
}